{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_MONTH = '2015_05_28'\n",
    "TEST_MONTH = '2016_05_28'\n",
    "\n",
    "TRAIN_FILE = 'data/train_' + TRAIN_MONTH + '.csv'\n",
    "ADDED_PRODUCTS_FILE = 'data/added_product_' + TRAIN_MONTH + '.csv'\n",
    "\n",
    "TEST_FILE = 'data/train_' + TEST_MONTH + '.csv'\n",
    "\n",
    "HEADER = [\"fecha_dato\", \"ncodpers\", \"ind_empleado\",\n",
    "          \"pais_residencia\", \"sexo\", \"age\", \"fecha_alta\",\n",
    "          \"ind_nuevo\", \"antiguedad\", \"indrel\", \"ult_fec_cli_1t\",\n",
    "          \"indrel_1mes\", \"tiprel_1mes\", \"indresi\", \"indext\",\n",
    "          \"conyuemp\", \"canal_entrada\", \"indfall\", \"tipodom\",\n",
    "          \"cod_prov\", \"nomprov\", \"ind_actividad_cliente\",\n",
    "          \"renta\", \"segmento\", \"ind_ahor_fin_ult1\",\n",
    "          \"ind_aval_fin_ult1\", \"ind_cco_fin_ult1\",\n",
    "          \"ind_cder_fin_ult1\", \"ind_cno_fin_ult1\",\n",
    "          \"ind_ctju_fin_ult1\", \"ind_ctma_fin_ult1\",\n",
    "          \"ind_ctop_fin_ult1\", \"ind_ctpp_fin_ult1\",\n",
    "          \"ind_deco_fin_ult1\", \"ind_deme_fin_ult1\",\n",
    "          \"ind_dela_fin_ult1\", \"ind_ecue_fin_ult1\",\n",
    "          \"ind_fond_fin_ult1\", \"ind_hip_fin_ult1\",\n",
    "          \"ind_plan_fin_ult1\", \"ind_pres_fin_ult1\",\n",
    "          \"ind_reca_fin_ult1\", \"ind_tjcr_fin_ult1\",\n",
    "          \"ind_valo_fin_ult1\", \"ind_viv_fin_ult1\",\n",
    "          \"ind_nomina_ult1\", \"ind_nom_pens_ult1\",\n",
    "          \"ind_recibo_ult1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_FILE, header=None, names=HEADER)\n",
    "test = pd.read_csv(TEST_FILE, header=None, names=HEADER)\n",
    "added_products = pd.read_csv(ADDED_PRODUCTS_FILE)\n",
    "\n",
    "combined = pd.concat((train, test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fixing age\n",
    "combined['age'] = pd.to_numeric(combined['age'], errors='coerce')\n",
    "# test['age'] = pd.to_numeric(test['age'], errors='coerce')\n",
    "\n",
    "combined.loc[combined.age < 18, \"age\"] = combined.loc[(combined.age > 18) & (combined.age <= 30), \"age\"].mean(skipna=True)\n",
    "combined.loc[combined.age > 100, \"age\"] = combined.loc[(combined.age > 30) & (combined.age <=100), \"age\"].mean(skipna=True)\n",
    "combined['age'].fillna(combined['age'].mean(), inplace=True)\n",
    "combined['age'] = combined['age'].astype(int)\n",
    "\n",
    "# fix ind_nuevo.. \n",
    "combined.loc[combined.ind_nuevo.isnull(), 'ind_nuevo'] = 1\n",
    "\n",
    "# fix antiguedad\n",
    "combined['antiguedad'] = pd.to_numeric(combined['antiguedad'], errors='coerce')\n",
    "combined.loc[combined.antiguedad.isnull(),'antiguedad'] = combined.antiguedad.min()\n",
    "combined.loc[combined.antiguedad < 0, 'antiguedad'] = 0\n",
    "\n",
    "# fix indrel\n",
    "combined.loc[combined.indrel.isnull(), 'indrel'] = 1\n",
    "\n",
    "# drop useless cols\n",
    "combined.drop(['tipodom', 'cod_prov'], axis=1, inplace=True)\n",
    "\n",
    "# fix ind_actividad_cliente\n",
    "# combined.ind_actividad_cliente = pd.to_numeric(combined.ind_actividad_cliente, errors='coerce')\n",
    "combined.loc[combined.ind_actividad_cliente.isnull(), \"ind_actividad_cliente\"] =\\\n",
    "    combined.ind_actividad_cliente.median()\n",
    "\n",
    "# fix city name\n",
    "combined.loc[combined.nomprov==\"CORU\\xc3\\x91A, A\",\"nomprov\"] = \"CORUNA, A\"\n",
    "combined.loc[combined.nomprov.isnull(), 'nomprov'] = 'UNKNOWN'\n",
    "\n",
    "#fix incomes\n",
    "# combined.renta = pd.to_numeric(combined.renta, errors='coerce')\n",
    "grouped = combined.groupby('nomprov').agg({'renta': lambda x: x.median(skipna=True)}).reset_index()\n",
    "new_incomes = pd.merge(combined, grouped, how='inner', on='nomprov').loc[:,['nomprov', 'renta_y']]\n",
    "\n",
    "new_incomes = new_incomes.rename(columns={\"renta_y\":\"renta\"}).sort_values(\"renta\").sort_values(\"nomprov\")\n",
    "\n",
    "combined.sort_values(\"nomprov\", inplace=True)\n",
    "combined = combined.reset_index()\n",
    "new_incomes = new_incomes.reset_index()\n",
    "combined.loc[combined.renta.isnull(), \"renta\"] = new_incomes.loc[combined.renta.isnull(), \"renta\"].median()\n",
    "combined.sort_values(by='fecha_dato', inplace = True)\n",
    "\n",
    "# rest of the columns\n",
    "string_data = combined.select_dtypes(include=[\"object\"])\n",
    "missing_columns = [col for col in string_data if string_data[col].isnull().any()]\n",
    "del string_data\n",
    "\n",
    "combined.loc[combined.indfall.isnull(), 'indfall'] = 'N'\n",
    "combined.loc[combined.tiprel_1mes.isnull(), 'tiprel_1mes'] = 'A'\n",
    "combined.tiprel_1mes = combined.tiprel_1mes.astype('category')\n",
    "\n",
    "map_dict = {\n",
    "    '1.0': '1',\n",
    "    '1': '1',\n",
    "    '3.0': '3',\n",
    "    'P': 'P',\n",
    "    3.0: '3',\n",
    "    2.0: '2',\n",
    "    '3': '3',\n",
    "    '2.0': '2',\n",
    "    '4.0': '4',\n",
    "    '4': '4',\n",
    "    '2': '2',\n",
    "    1.0: '1',\n",
    "    4.0: '4'\n",
    "}\n",
    "\n",
    "combined.indrel_1mes.fillna('P', inplace=True)\n",
    "combined.indrel_1mes = combined.indrel_1mes.apply(lambda x: map_dict[x])\n",
    "combined.indrel_1mes = combined.indrel_1mes.astype('category')\n",
    "\n",
    "unknown_cols = [col for col in missing_columns if col not in ['indfall', 'tiprel_1mes', 'indrel_1mes']]\n",
    "for col in unknown_cols:\n",
    "    combined.loc[combined[col].isnull(), col] = \"UNKNOWN\"\n",
    "\n",
    "# feature cols\n",
    "feature_cols = combined.iloc[:1,].filter(regex=\"ind_+.*ult.*\").columns.values\n",
    "for col in feature_cols:\n",
    "    combined.loc[combined[col].isnull(), col] = 0\n",
    "    combined[col] = combined[col].astype(int)\n",
    "\n",
    "del combined['ult_fec_cli_1t'], combined['fecha_alta']\n",
    "\n",
    "encoders = []\n",
    "for col in ['sexo', 'indrel_1mes','pais_residencia', 'ind_empleado', 'segmento', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada','indfall', 'nomprov']:\n",
    "    temp_enc = LabelEncoder()\n",
    "    temp_enc.fit(combined[col])\n",
    "    combined[col] = temp_enc.transform(combined[col])\n",
    "    encoders.append(temp_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = combined.loc[combined.fecha_dato == '2015-05-28', :].reset_index(drop=True)\n",
    "test = combined.loc[combined.fecha_dato == '2016-05-28', :].reset_index(drop=True)\n",
    "\n",
    "del train['index'], test['index']\n",
    "del train['fecha_dato'], test['fecha_dato']\n",
    "\n",
    "added_products.set_index('ncodpers', inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(added_products)\n",
    "added_products['encoded_products'] = label_encoder.transform(added_products['added_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.set_index('ncodpers', inplace=True)\n",
    "\n",
    "xTrain = train.loc[added_products.index, :]\n",
    "\n",
    "print train.shape\n",
    "print xTrain.shape\n",
    "print added_products.shape\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['eta'] = 0.05\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 0\n",
    "param['num_class'] = 22\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['min_child_weight'] = 2\n",
    "param['subsample'] = 0.9\n",
    "param['colsample_bytree'] = 0.9\n",
    "param['seed'] = 1428\n",
    "num_rounds = 100\n",
    "plist = param.items()\n",
    "\n",
    "xg_train = xgb.DMatrix(xTrain, label=added_products.loc[:, 'encoded_products'])\n",
    "evallist  = [(xg_train,'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.90573\n",
      "[1]\ttrain-mlogloss:2.77231\n",
      "[2]\ttrain-mlogloss:2.66007\n",
      "[3]\ttrain-mlogloss:2.56051\n",
      "[4]\ttrain-mlogloss:2.47907\n",
      "[5]\ttrain-mlogloss:2.4025\n",
      "[6]\ttrain-mlogloss:2.3346\n",
      "[7]\ttrain-mlogloss:2.27324\n",
      "[8]\ttrain-mlogloss:2.21839\n",
      "[9]\ttrain-mlogloss:2.17039\n",
      "[10]\ttrain-mlogloss:2.1228\n",
      "[11]\ttrain-mlogloss:2.07973\n",
      "[12]\ttrain-mlogloss:2.03906\n",
      "[13]\ttrain-mlogloss:2.00169\n",
      "[14]\ttrain-mlogloss:1.9667\n",
      "[15]\ttrain-mlogloss:1.93406\n",
      "[16]\ttrain-mlogloss:1.90349\n",
      "[17]\ttrain-mlogloss:1.87471\n",
      "[18]\ttrain-mlogloss:1.84758\n",
      "[19]\ttrain-mlogloss:1.82212\n",
      "[20]\ttrain-mlogloss:1.79804\n",
      "[21]\ttrain-mlogloss:1.7752\n",
      "[22]\ttrain-mlogloss:1.75334\n",
      "[23]\ttrain-mlogloss:1.73284\n",
      "[24]\ttrain-mlogloss:1.71348\n",
      "[25]\ttrain-mlogloss:1.69481\n",
      "[26]\ttrain-mlogloss:1.67756\n",
      "[27]\ttrain-mlogloss:1.66104\n",
      "[28]\ttrain-mlogloss:1.64494\n",
      "[29]\ttrain-mlogloss:1.62931\n",
      "[30]\ttrain-mlogloss:1.61459\n",
      "[31]\ttrain-mlogloss:1.60047\n",
      "[32]\ttrain-mlogloss:1.58696\n",
      "[33]\ttrain-mlogloss:1.57406\n",
      "[34]\ttrain-mlogloss:1.56158\n",
      "[35]\ttrain-mlogloss:1.54969\n",
      "[36]\ttrain-mlogloss:1.53829\n",
      "[37]\ttrain-mlogloss:1.52724\n",
      "[38]\ttrain-mlogloss:1.51658\n",
      "[39]\ttrain-mlogloss:1.50657\n",
      "[40]\ttrain-mlogloss:1.49708\n",
      "[41]\ttrain-mlogloss:1.48801\n",
      "[42]\ttrain-mlogloss:1.47902\n",
      "[43]\ttrain-mlogloss:1.4707\n",
      "[44]\ttrain-mlogloss:1.46231\n",
      "[45]\ttrain-mlogloss:1.45441\n",
      "[46]\ttrain-mlogloss:1.44681\n",
      "[47]\ttrain-mlogloss:1.43927\n",
      "[48]\ttrain-mlogloss:1.43193\n",
      "[49]\ttrain-mlogloss:1.42486\n",
      "[50]\ttrain-mlogloss:1.4181\n",
      "[51]\ttrain-mlogloss:1.41157\n",
      "[52]\ttrain-mlogloss:1.40534\n",
      "[53]\ttrain-mlogloss:1.39915\n",
      "[54]\ttrain-mlogloss:1.39317\n",
      "[55]\ttrain-mlogloss:1.38745\n",
      "[56]\ttrain-mlogloss:1.38187\n",
      "[57]\ttrain-mlogloss:1.37649\n",
      "[58]\ttrain-mlogloss:1.3713\n",
      "[59]\ttrain-mlogloss:1.36637\n",
      "[60]\ttrain-mlogloss:1.36141\n",
      "[61]\ttrain-mlogloss:1.35674\n",
      "[62]\ttrain-mlogloss:1.35212\n",
      "[63]\ttrain-mlogloss:1.34766\n",
      "[64]\ttrain-mlogloss:1.3433\n",
      "[65]\ttrain-mlogloss:1.33904\n",
      "[66]\ttrain-mlogloss:1.33486\n",
      "[67]\ttrain-mlogloss:1.33093\n",
      "[68]\ttrain-mlogloss:1.32703\n",
      "[69]\ttrain-mlogloss:1.32336\n",
      "[70]\ttrain-mlogloss:1.31967\n",
      "[71]\ttrain-mlogloss:1.31634\n",
      "[72]\ttrain-mlogloss:1.31296\n",
      "[73]\ttrain-mlogloss:1.30964\n",
      "[74]\ttrain-mlogloss:1.30643\n",
      "[75]\ttrain-mlogloss:1.30326\n",
      "[76]\ttrain-mlogloss:1.30005\n",
      "[77]\ttrain-mlogloss:1.2969\n",
      "[78]\ttrain-mlogloss:1.29384\n",
      "[79]\ttrain-mlogloss:1.29099\n",
      "[80]\ttrain-mlogloss:1.28821\n",
      "[81]\ttrain-mlogloss:1.28538\n",
      "[82]\ttrain-mlogloss:1.28262\n",
      "[83]\ttrain-mlogloss:1.28005\n",
      "[84]\ttrain-mlogloss:1.27754\n",
      "[85]\ttrain-mlogloss:1.27506\n",
      "[86]\ttrain-mlogloss:1.27254\n",
      "[87]\ttrain-mlogloss:1.27008\n",
      "[88]\ttrain-mlogloss:1.26774\n",
      "[89]\ttrain-mlogloss:1.26546\n",
      "[90]\ttrain-mlogloss:1.26316\n",
      "[91]\ttrain-mlogloss:1.26099\n",
      "[92]\ttrain-mlogloss:1.25888\n",
      "[93]\ttrain-mlogloss:1.25669\n",
      "[94]\ttrain-mlogloss:1.25463\n",
      "[95]\ttrain-mlogloss:1.25268\n",
      "[96]\ttrain-mlogloss:1.25078\n",
      "[97]\ttrain-mlogloss:1.24892\n",
      "[98]\ttrain-mlogloss:1.24713\n",
      "[99]\ttrain-mlogloss:1.2454\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(plist, xg_train, num_rounds, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.set_index('ncodpers', inplace=True)\n",
    "xg_test = xgb.DMatrix(test)\n",
    "\n",
    "preds = xgb_model.predict(xg_test)\n",
    "\n",
    "top_t_products = label_encoder.inverse_transform(np.argsort(preds, axis = 1)[:, ::-1][:, :])\n",
    "\n",
    "test['xgb_preds'] = [' '.join(x) for x in top_t_products]\n",
    "test['added_products'] = ['ind_recibo_ult1']*test.shape[0]\n",
    "\n",
    "for i in tqdm(test.index):\n",
    "    products = map(lambda x: x[0], filter(lambda x: x[1]==1, zip(HEADER[24:], test.loc[i, 'ind_ahor_fin_ult1':'ind_recibo_ult1'])))\n",
    "    pred_products = test.loc[i, 'xgb_preds'].split()\n",
    "    prod_string = ' '.join(filter(lambda x: x not in products, pred_products))\n",
    "    test.set_value(i, 'added_products', prod_string)\n",
    "\n",
    "submission = pd.read_csv('data/test_ver2.csv', usecols=[1])\n",
    "submission['added_products'] = ['ind_recibo_ult1']*submission.shape[0]\n",
    "submission.set_index('ncodpers', inplace=True)\n",
    "submission.added_products = test.loc[submission.index, 'added_products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing file\n",
      "Compression done\n",
      "Uploading submission data/submissions/my_first_xgb_sub_trained_on_jun_2015_only_added_users.csv.zip\n",
      "Upload done\n"
     ]
    }
   ],
   "source": [
    "from scripts.kaggle.helpers import make_submission\n",
    "filename = 'data/submissions/my_first_xgb_sub_trained_on_jun_2015_only_added_users.csv'\n",
    "description = 'my first xgboost submission on jun 2015 only added products data'\n",
    "submission.to_csv(filename, columns=['added_products'])\n",
    "make_submission(filename, description=description, submit=True, compress=True)\n",
    "# make_submission('data/submissions/2_bayesian_prob_products_counters_with_old_1_ignored.csv', description=\"2. based on bayesian probabilities, where probs are calculated only for prods when there is a transition from 0 to 1\", submit=True, compress=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
